{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a SFINCS model for FloodAdapt anywhere on the globe\n",
    "\n",
    "In this notebook we demonstrate how to setup a SFINCS model for the FloodAdapt backend at any given location. As an example we will use the Humber delta in the UK.\n",
    "\n",
    "## **Step 0:** Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import hydromt\n",
    "from hydromt_sfincs import SfincsModel\n",
    "from hydromt.log import setuplog\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "\n",
    "proj = ccrs.PlateCarree() # plot projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1a:** Configuration - List inputs\n",
    "\n",
    "Here we list some basic inputs, e.g. relevant names, input path, datasets.\n",
    "- `model_name`: This is the name for the overall FloodAdapt folder where everything will be stored. Named after region of interest (in this case)\n",
    "- `model_path`: The full path of the FloodAdapt model folder\n",
    "- `sf_root`: Path to folder parsed by FloodAdapt where SFINCS model is stored. \n",
    "- `sf_logger_name`: Name for the SFINCS logger\n",
    "- `region_fn`: Path to geojson file of the region of interest. Used when building SFINCS domain\n",
    "- `tref`: SFCINS reference time\n",
    "- `tstart`: SFINCS start time\n",
    "- `tstop`: SFINCS stop time\n",
    "\n",
    "The SFINCS start and stop time can later be overwritten through scenarios. The values given here serve as a default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Humber'\n",
    "model_path = Path('C:/Repositories/DT-flood/FloodAdapt_database') / model_name\n",
    "\n",
    "sf_root = model_path / Path('static/templates/overland')\n",
    "\n",
    "sf_logger_name = 'SFINCS_Logger'\n",
    "\n",
    "region_fn = model_path / 'HumberDelta_large.geojson'\n",
    "\n",
    "tref = \"20131201 000000\"\n",
    "tstart = \"20131205 000000\"\n",
    "tstop = \"20131207 000000\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `data_dict` as a central object for input data. The dict values are currently the dataset names used in the deltares_data data_catalog. When changing datacatalog, make sure to also change the names here. Using file paths as dict values should also work.\n",
    "\n",
    "In this particular example we use only global data or models as input. For better results use local data from the region of interest whenever available\n",
    "\n",
    "The `meteo` key is used when precipitation, airpressure, and wind velocities are bundled in one data set (e.g. in the case of ERA5). When using separate dataset for these three, use the `precip`, `pressure`, and `winds` keys respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for own data_catalog file\n",
    "# data_catalog_fn = model_path / 'sfincs_catalog.yml'\n",
    "\n",
    "data_dict = {\n",
    "    'topo': 'copdem30',\n",
    "    'bathy': 'gebco',\n",
    "    'waterlevels': 'gtsm_codec_reanalysis_hourly_v1',\n",
    "    'meteo': 'era5_hourly',\n",
    "    'infiltration': 'gcn250',\n",
    "    'lulc': 'esa_worldcover'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1b:** Configuration - Initialize tools\n",
    "\n",
    "Here we initialize some useful tools:\n",
    "- `sf_logger`: logger object keeping track of changes to SFINCS model\n",
    "- `data_catalog`: The data_catalog containing the data we want to use. Here the deltares_data catalog\n",
    "- `sf`: SFINCS model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_logger = setuplog(sf_logger_name,log_level=10)\n",
    "\n",
    "data_catalog = hydromt.DataCatalog(data_libs=['deltares_data'],logger=sf_logger)\n",
    "\n",
    "sf = SfincsModel(data_libs=['deltares_data'], root=sf_root, mode='w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2a:** Model building - Domain\n",
    "\n",
    "Here we configure the model domain based on the geojson file given in `region_fn`.\n",
    "\n",
    "In general we don't assume the provided geojson geometry to conform nicely to any geographical/hydrographical features, something that is desirable for the final model domain. We fix this by looking up all the river basins that lie (partially) in the provided geometry. The Hydrosheds Basin Atlas is a good global source for watersheds.\n",
    "\n",
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = gpd.read_file(region_fn)\n",
    "\n",
    "ds_basins = data_catalog.get_geodataframe(\n",
    "    'basin_atlas_level12_v10',\n",
    "    geom=region\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the initially specified region (black) and the SFINCS domain (blue)\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax= plt.subplot(projection=proj)\n",
    "ax.add_image(cimgt.QuadtreeTiles(),12)\n",
    "ds_basins['geometry'].boundary.plot(ax=ax)\n",
    "ax.set_extent(ds_basins['geometry'].total_bounds[[0,2,1,3]], crs=proj)\n",
    "region.boundary.plot(ax=ax,color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2b:** Model building - The Grid\n",
    "\n",
    "Next we build the model grid in our domain. This will start as the bounding box of the domain, but we will refine this later by specifying active cells. Any input we provide will automatically be converted to the `crs` we specify here. The resolution `res` is in units of the `crs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_grid_from_region(\n",
    "    region = {'bbox': ds_basins['geometry'].total_bounds},\n",
    "    res = 200,\n",
    "    rotated=True,\n",
    "    crs='utm'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model region\n",
    "_ = sf.plot_basemap(plot_region=True,bmap='sat',zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevation data\n",
    "\n",
    "Refining the grid is generally done based on elevation data, so first we need to load those in. By using the `elevtn` key in the dictionary we tell the *setup_dep()* function to look for that variable in the specified dataset. It is possible to provide multiple datasets for the elevation data as we do below. The second data set will fill in the gaps of the first one, or we can manually tell it to switch at a given elevation, done here by providing a `zmin` for the first data set.\n",
    "  \n",
    "Here we use global data sets, Copernicus DEM for topography (resolution 1arcsec ~ 30m) and GEBCO (resolution 450m) for bathymetry. When available for your region we highly recommend using more high resolution data (in this case particularly for bathymetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dep = [{'elevtn': data_dict['topo'], 'zmin': 0.001}, {'elevtn': data_dict['bathy']}]\n",
    "_ = sf.setup_dep(datasets_dep=datasets_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sf.plot_basemap(variable='dep',bmap='sat',zoomlevel=10, plot_region=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active cells\n",
    "\n",
    "Next we refine the grid by specifying which cells are active by calling the function *setup_mask_active()*. Active cells can be defined through a minimum and maximum height they should have (with `zmin` and `zmax`) or by providing a mask. The argument `reset_mask` specifies whether to override the existing mask, of which we have to be mindful in this case as we define our active cells with multiple calls.\n",
    "\n",
    "In our example the mask of active cells consists of two parts: one on open water (defined by a `zmin` of -25m and `zmax` of 0m) and one on land defined by the domain we created earlier (using `include_mask`). Finally we plug any gaps that might exist between the two with a final call providing only the `fill_area` argument. This will turn any isolated patches of inactive cells smaller than some specified area (10km<sup>2</sup> in this case) active. This is usually done automatically for patches smaller than a default value, but due to the particular ordering inside the *setup_mask_active* function we need fill the gaps with an additional call.\n",
    "\n",
    "\n",
    "*Note: The value of -25m for `zmin` is much lower than what is typically used (-2 or -5m). The reason is that we want to include the full Humber estuary which is much deeper than -5m*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_active = gpd.GeoDataFrame(geometry=ds_basins['geometry'])\n",
    "\n",
    "sf.setup_mask_active(zmax=0, zmin=-25, include_mask=mask_active, reset_mask=True)\n",
    "sf.setup_mask_active(fill_area=10, reset_mask=False)\n",
    "\n",
    "_ = sf.plot_basemap(variable='msk', bmap='sat', zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waterlevel bound\n",
    "\n",
    "The boundaries function by default as hard walls, allowing no flow in or out. We want to use the boundaries on open water to force the water levels there. We can change what type of cell they are by calling *setup_mask_bounds* and providing the argument `btype` with the desired cell type. Setting `btype` to *waterlevel* will specify waterlevel boundaries, where we provide the additional condition that those cells have to be below -1m with `zmax`.\n",
    "\n",
    "If you suspect any significant flow to outside the model to occur at some boundaries, you can set those boundaries as outflow boundaries by setting `btype` to *outflow* and providing the geometry of the desired bounds. This is usually only done as a refinement step after the model has run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_mask_bounds(btype='waterlevel', zmax=-2, reset_bounds=True)\n",
    "\n",
    "_ = sf.plot_basemap(variable='msk', bmap='sat', zoomlevel=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2c:** Model building - Forcing\n",
    "\n",
    "Next we configure the forcing, the data we use as input in the model. These are the water levels at the waterlevel boundaries, the rainfall, and the river discharges (although we won't use this in our example). We need to provide a time window to simulate to get the right data. For our example we pick December 5th through 7th 2013 when the Humber delta flooded due to a tidal surge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sim Time\n",
    "\n",
    "Set up the simulation times. This is also used when loading in the meteo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_config(\n",
    "    **{\n",
    "        \"tref\": tref,\n",
    "        \"tstart\": tstart,\n",
    "        \"tstop\": tstop,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waterlevels at boundary\n",
    "\n",
    "For the forcing of the water levels at the boundary we use the Global Tide and Surge Model (GTSM). This includes hourly waterlevels at predetermined points. To get the waterlevels at our boundary we interpolate these points to our boundary when we read in the data. The `buffer` we provide specifies the distance from our boundary to look for points in the GTSM dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_waterlevel_forcing(geodataset=data_dict['waterlevels'],buffer=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteo\n",
    "\n",
    "For all the meteorological data we use the hourly ERA5 reanalysis data. This particular dataset requires some renaming of variables to be able to be parsed by the model builder. When providing your own data please check the variable naming conventions in the documentation.\n",
    "\n",
    "Additionally we have set up the cell below so that the ERA5 dataset will function as a default even when incomplete meteorological data is provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sets era5 hourly as a default, even when some categories are not provided\n",
    "era5 = data_catalog.get_rasterdataset('era5_hourly', geom=sf.geoms['region'],time_tuple=[sf.config['tstart'],sf.config['tstop']])\n",
    "era5 = era5.rename({'wind10_u': 'wind_u', 'wind10_v': 'wind_v', 'press_msl': 'press'})\n",
    "sf.setup_precip_forcing_from_grid(precip=era5['precip'], aggregate=False)\n",
    "sf.setup_wind_forcing_from_grid(era5[['wind_u', 'wind_v']])\n",
    "sf.setup_pressure_forcing_from_grid(100*era5['press'])\n",
    "\n",
    "if 'meteo' in data_dict and data_dict['meteo'] != 'era5_hourly':\n",
    "    meteo_data = data_catalog.get_rasterdataset(\n",
    "        data_dict['meteo'],\n",
    "        geom=sf.geoms['region'],\n",
    "        time_tuple=[sf.config['tstart'],sf.config['tstop']]\n",
    "    )\n",
    "\n",
    "    sf.setup_precip_forcing_from_grid(precip=meteo_data['precip'])\n",
    "    sf.setup_wind_forcing_from_grid(meteo_data[['wind_u', 'wind_v']])\n",
    "    sf.setup_precip_forcing_from_grid(meteo_data['press'])\n",
    "\n",
    "if 'precip' in data_dict:\n",
    "    if data_dict['precip'].endswith('.csv'):\n",
    "        sf.setup_precip_forcing(data_dict['precip'])\n",
    "    else:\n",
    "        sf.setup_precip_forcing_from_grid(data_dict['precip'])\n",
    "if 'pressure' in data_dict:\n",
    "    sf.setup_pressure_forcing_from_grid(data_dict['pressure'])\n",
    "if 'winds' in data_dict:\n",
    "    if data_dict['winds'].endswith('.csv'):\n",
    "        sf.setup_wind_forcing(data_dict['winds'])\n",
    "    else:\n",
    "        sf.setup_wind_forcing_from_grid(data_dict['winds'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sf.plot_forcing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land roughness and infiltration\n",
    "\n",
    "For the flow of water on land we need to know how much water gets absorped into the soil and how ruogh the surface is.\n",
    "\n",
    "For the soil infiltration we use global curve numbers indicating hwo much water will run off or be absorped based on soil types and antecedent moisture levels.\n",
    "\n",
    "For the surface roughness we specify land uses, which are automatically converted into manning roughness values. Any gaps in the dataset will be plugged with default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup soil infiltration\n",
    "sf.setup_cn_infiltration(data_dict['infiltration'], antecedent_moisture='avg')\n",
    "\n",
    "# Setup surface roughness\n",
    "datasets_rgh = [{'lulc': data_dict['lulc']}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2d:** Model Building - The Subgrid\n",
    "\n",
    "So far our model uses the grid we specified with the resolution we specified, in this case 200m. This means we are not making optimal use of our DEM as a grid cell is much larger than the DEM resolution. Increasing the grid resolution however is not a good idea as the computing time scales with the 3rd power of the grid size.\n",
    "\n",
    "Instead we make use of a so-called subgrid. The idea is that for a given water level in a grid cell we calculate a more realistic water volume in the cell using the high-res DEM. The water flow is only calculated between regular grid cells but using these new water volumes. Computationally this is much more efficient.\n",
    "\n",
    "To setup the subgrid we need to provide the relevant topographical data. These are the high-res DEM, the roughness data (not strictly needed, a default will be used when no argument is provided), and river data (although we don't use this in our example). Finally we need to specify the number of subgrid cells in each grid cell (lateral size, total nr of subgrid pixels per cell is `nr_subgrid_pixels`<sup>2</sup>). This we want to match our DEM resolution as close as possible. In our example the grid resolution is 200m while the DEM resolution is 1arcsec ~ 30m, so we will use `nr_subgrid_pixels` = 6, as 200m/6 is slightly more than 30m.\n",
    "\n",
    "Finally we write the elevation subgrid data, which is later used by FloodAdapt.\n",
    "\n",
    "Creating the subgrid may take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.setup_subgrid(\n",
    "    datasets_dep=datasets_dep,  # The elevation data\n",
    "    datasets_rgh=datasets_rgh,  # The roughness data\n",
    "    nr_subgrid_pixels=6,\n",
    "    write_dep_tif=True,         # Whether to write the subgrid elevation data\n",
    "    write_man_tif=False         # Whether to write the subgrid roughness data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sf.plot_basemap(variable='subgrid.z_zmin',bmap='sat',zoomlevel=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write model to appropriate folders\n",
    "\n",
    "Save all the model files to `sf_root`. This particular folder structure is parsed later by FloodAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree(directory):\n",
    "    print(f\"+ {directory}\")\n",
    "    for path in sorted(directory.rglob('*')):\n",
    "        depth = len(path.relative_to(directory).parts)\n",
    "        spacer = \"  \" * depth\n",
    "        print(f\"{spacer}+ {path.name}\")\n",
    "\n",
    "\n",
    "tree(Path(sf.root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
